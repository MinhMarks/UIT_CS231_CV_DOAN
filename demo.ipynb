{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15af71f-58aa-4d0b-b2c6-a1f637e40062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter-iec_23se07/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/tmp/ipykernel_352259/1008000905.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 17.50 MiB is free. Process 94290 has 1.53 GiB memory in use. Process 338791 has 4.15 GiB memory in use. Process 347769 has 1.91 GiB memory in use. Including non-PyTorch memory, this process has 172.00 MiB memory in use. Of the allocated memory 38.07 MiB is allocated by PyTorch, and 3.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 151\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    149\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs/lightning_logs/version_60/checkpoints/last.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNordland\u001b[39m\u001b[38;5;124m'\u001b[39m] :\n\u001b[1;32m    154\u001b[0m         val_dataset, num_references, num_queries, ground_truth \u001b[38;5;241m=\u001b[39m get_val_dataset(val_name, [\u001b[38;5;241m126\u001b[39m, \u001b[38;5;241m126\u001b[39m] )\n",
      "Cell \u001b[0;32mIn[4], line 108\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(ckpt_path)\u001b[0m\n\u001b[1;32m    105\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    107\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 108\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py:54\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 17.50 MiB is free. Process 94290 has 1.53 GiB memory in use. Process 338791 has 4.15 GiB memory in use. Process 347769 has 1.91 GiB memory in use. Including non-PyTorch memory, this process has 172.00 MiB memory in use. Of the allocated memory 38.07 MiB is allocated by PyTorch, and 3.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "from vpr_model import VPRModel\n",
    "import faiss\n",
    "import faiss.contrib.torch_utils\n",
    "from prettytable import PrettyTable\n",
    "# Dataloader\n",
    "from dataloaders.val.NordlandDataset import NordlandDataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from dataloaders.val.MapillaryDataset import MSLS\n",
    "# from dataloaders.val.MapillaryTestDataset import MSLSTest\n",
    "# from dataloaders.val.PittsburghDataset import PittsburghDataset\n",
    "# from dataloaders.val.SPEDDataset import SPEDDataset\n",
    "\n",
    "# VAL_DATASETS = ['MSLS', 'MSLS_Test', 'pitts30k_test', 'pitts250k_test', 'Nordland', 'SPED']\n",
    "VAL_DATASETS = ['Nordland']\n",
    "\n",
    "\n",
    "def input_transform(image_size=None):\n",
    "    MEAN=[0.485, 0.456, 0.406]; STD=[0.229, 0.224, 0.225]\n",
    "    if image_size:\n",
    "        return T.Compose([\n",
    "            T.Resize(image_size,  interpolation=T.InterpolationMode.BILINEAR),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "    else:\n",
    "        return T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "\n",
    "def get_val_dataset(dataset_name, image_size=None):\n",
    "    dataset_name = dataset_name.lower()\n",
    "    transform = input_transform(image_size=image_size)\n",
    "    \n",
    "    if 'nordland' in dataset_name:    \n",
    "        ds = NordlandDataset(input_transform=transform)\n",
    "    # elif 'msls_test' in dataset_name:\n",
    "    #     ds = MSLSTest(input_transform=transform)\n",
    "\n",
    "    # elif 'msls' in dataset_name:\n",
    "    #     ds = MSLS(input_transform=transform)\n",
    "\n",
    "    # elif 'pitts' in dataset_name:\n",
    "    #     ds = PittsburghDataset(which_ds=dataset_name, input_transform=transform)\n",
    "\n",
    "    # elif 'sped' in dataset_name:\n",
    "    #     ds = SPEDDataset(input_transform=transform)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    num_references = ds.num_references\n",
    "    num_queries = ds.num_queries\n",
    "    ground_truth = ds.ground_truth\n",
    "    # print( \"==================  label of some place ==================== \")  \n",
    "    # print( ground_truth[:5][1] ) \n",
    "    return ds, num_references, num_queries, ground_truth\n",
    "\n",
    "def get_descriptors(model, dataloader, device):\n",
    "    descriptors = []\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            for batch in tqdm(dataloader, 'Calculating descritptors...'):\n",
    "                imgs, labels = batch\n",
    "                output = model(imgs.to(device)).cpu()\n",
    "                descriptors.append(output)\n",
    "\n",
    "    return torch.cat(descriptors)\n",
    "\n",
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Giúp khôi phục ảnh từ dạng normalized về dạng hiển thị được (0-1 range).\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # t = t * std + mean\n",
    "    return torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "def load_model(ckpt_path):\n",
    "    model = VPRModel(\n",
    "        backbone_arch='dinov2_vitb14',\n",
    "        backbone_config={\n",
    "            'num_trainable_blocks': 4,\n",
    "            'return_token': True,\n",
    "            'norm_layer': True,\n",
    "        },\n",
    "        agg_arch='SALAD',\n",
    "        agg_config={\n",
    "            'num_channels': 768,\n",
    "            'num_clusters': 64,\n",
    "            'cluster_dim': 128,\n",
    "            'token_dim': 256,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    \n",
    "    # Lấy đúng phần state_dict bên trong\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n",
    "    model = model.eval()\n",
    "    model = model.to('cuda')\n",
    "    print(f\"Loaded model from {ckpt_path} successfully!\")\n",
    "    return model\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Eval VPR model\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    # Model parameters\n",
    "    parser.add_argument(\"--ckpt_path\", type=str, required=True, default=None, help=\"Path to the checkpoint\")\n",
    "    \n",
    "    # Datasets parameters\n",
    "    parser.add_argument(\n",
    "        '--val_datasets',\n",
    "        nargs='+',\n",
    "        default=VAL_DATASETS,\n",
    "        help='Validation datasets to use',\n",
    "        choices=VAL_DATASETS,\n",
    "    )\n",
    "    parser.add_argument('--image_size', nargs='*', default=None, help='Image size (int, tuple or None)')\n",
    "    parser.add_argument('--batch_size', type=int, default=512, help='Batch size')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Parse image size\n",
    "    if args.image_size:\n",
    "        if len(args.image_size) == 1:\n",
    "            args.image_size = (args.image_size[0], args.image_size[0])\n",
    "        elif len(args.image_size) == 2:\n",
    "            args.image_size = tuple(args.image_size)\n",
    "        else:\n",
    "            raise ValueError('Invalid image size, must be int, tuple or None')\n",
    "        \n",
    "        args.image_size = tuple(map(int, args.image_size))\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660c351-88ac-4a74-95b8-549bf56c8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = load_model('./logs/lightning_logs/version_60/checkpoints/last.ckpt')\n",
    "\n",
    "val_dataset, num_references, num_queries, ground_truth = get_val_dataset('Nordland', [126, 126] )\n",
    "val_loader = DataLoader('Nordland', num_workers=16, batch_size=16, shuffle=False, pin_memory=True)\n",
    "\n",
    "    print(f'Evaluating on {val_name}')\n",
    "    descriptors = get_descriptors(model, val_loader, 'cuda')\n",
    "    \n",
    "    print(f'Descriptor dimension {descriptors.shape[1]}')\n",
    "    r_list = descriptors[ : num_references]\n",
    "\n",
    "    print('total_size', descriptors.shape[0], num_queries + num_references) \n",
    "\n",
    "    # descriptors dimension is 8448 \n",
    "    # testing = True # isinstance(val_dataset, MSLSTest)\n",
    "\n",
    "    preds = get_validation_recalls(\n",
    "        r_list=r_list,\n",
    "        q_list=q_list,\n",
    "        k_values=[1, 5, 10, 15, 20, 25],\n",
    "        gt=ground_truth,\n",
    "        print_results=True,\n",
    "        dataset_name=val_name,\n",
    "        faiss_gpu=False,\n",
    "        testing=False,\n",
    "    )\n",
    "\n",
    "    embed_size = r_list.shape[1]\n",
    "        if faiss_gpu:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            flat_config = faiss.GpuIndexFlatConfig()\n",
    "            flat_config.useFloat16 = True\n",
    "            flat_config.device = 0\n",
    "            faiss_index = faiss.GpuIndexFlatL2(res, embed_size, flat_config)\n",
    "        # build index\n",
    "        else:\n",
    "            faiss_index = faiss.IndexFlatL2(embed_size)\n",
    "\n",
    "        # add references\n",
    "        faiss_index.add(r_list)\n",
    "\n",
    "        # search for queries in the index\n",
    "        _, predictions = faiss_index.search(q_list, max(k_values))\n",
    "\n",
    "    # Chọn 2 query để hiển thị\n",
    "    \n",
    "    # for q_idx in [1000, 1]:\n",
    "    #     print(f\"\\nQuery index: {q_idx}\")\n",
    "    #     pred_ids = preds[q_idx][:4]  # top-4 predictions\n",
    "    \n",
    "    #     # Lấy ảnh query và reference từ dataset (tensor), rồi denormalize để hiển thị\n",
    "    #     query_img, _ = val_dataset[num_references + q_idx]\n",
    "    #     query_img = denormalize(query_img.clone())\n",
    "    #     query_img_np = TF.to_pil_image(query_img)\n",
    "    \n",
    "    #     fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    #     axes[0].imshow(query_img_np)\n",
    "    #     axes[0].set_title(\"Query\")\n",
    "    #     axes[0].axis('off')\n",
    "    \n",
    "    #     for i, pred_id in enumerate(pred_ids):\n",
    "    #         ref_img, _ = val_dataset[pred_id]\n",
    "    #         ref_img = denormalize(ref_img.clone())\n",
    "    #         ref_img_np = TF.to_pil_image(ref_img)\n",
    "    \n",
    "    #         axes[i+1].imshow(ref_img_np)\n",
    "    #         axes[i+1].set_title(f\"Top-{i+1}\")\n",
    "    #         axes[i+1].axis('off')\n",
    "    \n",
    "    #     plt.tight_layout()\n",
    "    #     plt.savefig(f\"./query_{q_idx}_top4.png\")\n",
    "    #     plt.close()\n",
    "\n",
    "\n",
    "    # print( preds ) \n",
    "    \n",
    "    # if testing:\n",
    "    #     val_dataset.save_predictions(preds, '../preds.txt')\n",
    "\n",
    "    del descriptors\n",
    "    print('========> DONE!\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
